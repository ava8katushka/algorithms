
import Image from 'next/image'
import YoutubeShort from "../components/YoutubeShort";
import Code from "../components/Code";

You can read, or you can watch the video:

<YoutubeShort code="8ZjgN0POkEo?si=pRSJIv_XUvBRSLiB"/>

# Demystifying Time and Space Complexity for Beginners

Hi there! Welcome—I'm so glad you're here.
My name's Ava, and I’ll be your guide through Algorithms and Data Structures.
A big thanks to everyone who joined our [Linkedin Group](https://www.linkedin.com/groups/12837112/).
I’m thrilled we’re starting this journey together!

Today, we’re diving into **time and space** complexity—an essential topic
for understanding how efficient your programs are.
Let’s break it down step by step.


## What is n?  

You’ve probably heard terms like “O(n),” “n log n,” or “O(n²)” being tossed around.
But what does this mysterious n mean?

When writing programs, **n** usually refers to the **size of the input**. It could be:

- The length of an array or string.
- The number of elements in a data structure.
- Any input size your program processes.

For example, if we have a function that multiplies all elements in an array, the size of the array is n:
<Code>
{

`# Length of the array is "n"
def multiply(arr): 
    result = 1
    for a in arr:
        result *= a
    return result
`
}
</Code>

Here, the time it takes to run the function depends on **n**, the size of the input array.

## What is time complexity? 

Time complexity measures how the runtime of your program changes
as the size of the input (n) grows.
Think of it as a way to estimate efficiency.
Let’s explore a few common types:


### O(1): Constant Time Complexity

The best-case scenario!
Here, the runtime is independent of the input size.
Whether the input has 10 elements or 10 billion,
 it takes the same amount of time.

Example:

<Code>
{
`a = 0
b += 1`
}
</Code>


# O(n): Linear Time Complexity

With linear complexity, the runtime grows in **direct proportion** to the input size. If the input size doubles, the runtime roughly doubles.

**Example 1: Summing an Array**

If you’re adding up all elements in an array, your program needs to visit each element one by one. This results in O(n) time complexity:
<Code>
{
`def sum_array(arr):
    total = 0
     # Loop runs once for each element in arr
    for num in arr: 
        total += num
    return total
`
}
</Code>

**Example 2: Searching in an Unsorted Array**

Suppose you’re searching for a specific value in an unsorted array.
In the worst case, you’ll have to check every element until you find it
 (or confirm it’s not there).

<Code>
{
`def search(arr, target):
 # Loop runs n times if the target isn’t found
    for num in arr:
        if num == target:
            return True
    return False
`
}
</Code>


# O(n²): Quadratic Time Complexity

Quadratic complexity arises when your program performs nested operations— for example, when one loop is inside another.
If the input size increases by **n**, the number of operations increases by **n²**.

**Example: Processing a Matrix**

Imagine summing all the elements in a 2D matrix.

<Code>
{
`def sum_matrix(matrix):
    total = 0
    # Outer loop runs n times (rows)
    for i in range(len(matrix)):
    # Inner loop runs n times (columns)  
        for j in range(len(matrix[0])):  
            total += matrix[i][j]
    return total
`
}
</Code>

Here, if the matrix size is **n × n**, the total number of iterations is **n × n = n²**.

## O(2ⁿ): Exponential Time Complexity


Exponential time complexity means the runtime of your program **doubles**
 (or grows even faster) with **every additional input element**.

 For **n = 10** you would do **1024** iterations. For **n = 100** it's **1267650600228229401496703205376**!

Exponential algorithms are highly inefficient for large inputs,
making them impractical except for very small datasets.

**Example 1: The Fibonacci Sequence (Naive Recursion)**

A classic example is calculating the nth Fibonacci number using a naive recursive function.
The function calls itself twice for each input, resulting in exponential growth:

<Code>
{
`def fibonacci(n):
    if n <= 1:
        return n
    return fibonacci(n - 1) + fibonacci(n - 2)
`
}
</Code>

Here, the number of recursive calls grows exponentially as **2ⁿ**, making it painfully slow for large **n**.

# Constants in Complexity: Why O(5n) = O(n)

When discussing time complexity, constants don’t matter. O(n), O(5n), and O(500n) are all considered O(n) because we care about the **growth rate**, not the exact runtime.

That said, constants *do* matter in real life, but for now, we’re focusing on the big picture.

## What is space complexity?

Space complexity measures how much memory your program uses as the input grows. There are two main contributors to space complexity:

1. **Variables and Data Structures**
    
    If you create an array, list, or matrix with **n** elements, that’s **O(n)** space complexity.
<Code>
{
`def do_something(arr):
    # Constant space: O(1)
    a = 5
    # Linear space: O(n)
    new_arr = copy(arr)`
}
</Code>
2. **The Recursive Call Stack**

    Recursion requires additional memory to keep track of function calls. Each time a function calls itself, the program uses space to store:
        - Where to return after the call.
        - What arguments were passed.

The deeper the recursion, the more memory it consumes.

Illustration:

<Image
      src="https://res.cloudinary.com/ava-coding-com/image/upload/v1737832285/Algorithms Book/RecursionStack_gb8yr3.png"
      width={420}
      height={592}
      alt="Depth of stack example"
/>

## Want a handy reference?
You can download this **Time and Space Complexity Poster** to keep the concepts fresh:

[Download original](https://res.cloudinary.com/ava-coding-com/image/upload/v1647240152/Algorithms%20Book/1st_opening_ooh6yr.png)
<Image
      src="https://res.cloudinary.com/ava-coding-com/image/upload/v1647240152/Algorithms%20Book/1st_opening_ooh6yr.png"
      width={1000}
      height={500}
      alt="Time and Space Complexities Poster"
/>

## Why Does Complexity Matter?

You might be thinking: “Computers are so fast these days—why should I care about complexity?”

Great question! And it’s one we’ll answer in the next lesson. I’ll teach you how competitive programmers predict runtimes using simple tricks. It’s a game-changer for solving problems efficiently.

Stay tuned, and I’ll see you next week!